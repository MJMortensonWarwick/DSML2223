{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBKH05uPYciT1ATcfvY1ZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/DSML2223/blob/main/1_1_Regression_(Stats_vs_ML).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression in Scikit-Learn\n",
        "\n",
        "### Traditional Statistics Approach\n",
        "As above, its a Linear Regression tutorial (yay), using the popular (indeed some may say seminal) machine learning library scikit-learn. As per usual, we will start by installing and importing:"
      ],
      "metadata": {
        "id": "SQaQTTM8HkIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsZu7TTCHVTF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd  \n",
        "import seaborn as sns \n",
        "\n",
        "# Only works on Jupyter/Anaconda\n",
        "%matplotlib inline  \n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also use one of the inbuilt datasets included with _scikit\\-learn_:"
      ],
      "metadata": {
        "id": "iRyyQotGIHTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import a standard dataset - the Boston house price index\n",
        "from sklearn.datasets import load_boston\n",
        "boston_dataset = load_boston()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPefRRmqIKfa",
        "outputId": "747a0297-6816-4c32-dfc2-0b903f183016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the comment says, this is a well known dataset ... indeed Andrew Ng's first dataset in his first lecture of the first mainstream MOOC in machine learning. Let's look at it:"
      ],
      "metadata": {
        "id": "3qWvUfZNIOJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show the dataset\n",
        "print(boston_dataset)\n",
        "\n",
        "# print a return space\n",
        "print('\\n')\n",
        "\n",
        "# show the keys\n",
        "print(boston_dataset.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WzUax3AISeU",
        "outputId": "802c0bdd-525e-4b9c-ebbc-ab0553311638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
            "        4.9800e+00],\n",
            "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
            "        9.1400e+00],\n",
            "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
            "        4.0300e+00],\n",
            "       ...,\n",
            "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
            "        5.6400e+00],\n",
            "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
            "        6.4800e+00],\n",
            "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
            "        7.8800e+00]]), 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
            "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
            "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
            "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
            "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
            "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
            "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
            "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
            "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
            "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
            "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
            "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
            "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
            "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
            "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
            "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
            "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
            "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
            "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
            "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
            "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
            "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
            "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
            "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
            "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
            "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
            "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
            "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
            "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
            "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
            "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
            "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
            "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
            "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
            "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
            "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
            "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
            "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
            "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
            "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
            "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
            "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
            "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
            "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
            "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
            "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
            "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'), 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\", 'filename': 'boston_house_prices.csv', 'data_module': 'sklearn.datasets.data'}\n",
            "\n",
            "\n",
            "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see this is actually a dictionary with 'data' (the X's we can use), 'target' (the Y's), 'feature_names' (the names of all of the columns that represent X), 'DESCR' (a description of the data) and 'filename' (I'll let you guess). With this in mind let's add the X's to a DataFrame:"
      ],
      "metadata": {
        "id": "zCjjVtOoIWMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to data frame using Pandas\n",
        "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "boston.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2qL857JTIcAo",
        "outputId": "9fbfa668-e73a-48e7-9dbb-e0f8b77f3a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
              "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
              "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
              "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
              "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
              "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
              "\n",
              "   PTRATIO       B  LSTAT  \n",
              "0     15.3  396.90   4.98  \n",
              "1     17.8  396.90   9.14  \n",
              "2     17.8  392.83   4.03  \n",
              "3     18.7  394.63   2.94  \n",
              "4     18.7  396.90   5.33  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86555453-e7b1-47b2-add8-164c3dd0b773\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86555453-e7b1-47b2-add8-164c3dd0b773')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86555453-e7b1-47b2-add8-164c3dd0b773 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86555453-e7b1-47b2-add8-164c3dd0b773');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also create a variable for the Y value:"
      ],
      "metadata": {
        "id": "nX2xbkuMIjAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a separate Y value\n",
        "boston_Y = boston_dataset.target\n",
        "boston_Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2antsWmIluJ",
        "outputId": "6f3cb540-983e-4eec-ff2b-801188b7ae9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
              "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
              "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
              "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
              "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
              "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
              "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
              "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
              "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
              "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
              "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
              "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
              "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
              "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
              "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
              "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
              "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
              "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
              "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
              "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
              "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
              "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
              "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
              "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
              "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
              "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
              "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
              "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
              "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
              "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
              "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
              "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
              "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
              "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
              "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
              "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
              "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
              "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
              "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
              "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
              "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
              "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
              "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
              "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
              "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
              "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here we can build our model. Let's recall that the model we want is of the form:\n",
        "\n",
        "$ Y = a + b_1x_1 + b_2x_2 + [...] + b_13x_13 + e$\n",
        "\n",
        "Where \n",
        "$Y$ is our target variable (boston_Y), $a$ is the intercept, the various $b$ values (1 to 13) represent the effect of the corresponding $x$ values (1 to 13) on the slope and $e$ is the error.\n",
        "\n",
        "We start by defining a model and 'fitting' it to the data:"
      ],
      "metadata": {
        "id": "FWQvuZrLIohs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_model = LinearRegression()\n",
        "\n",
        "# fit the model to the training data\n",
        "lin_model_fit = lin_model.fit(boston, boston_Y)"
      ],
      "metadata": {
        "id": "2RWgZvWbJQRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have created a Linear Regression object (lin_model) and then an object based on fitting it to our data. In other words, lin_model_fit is the Linear Regression process applied to our data.\n",
        "\n",
        "From this object we can establish the remainder of our formula:"
      ],
      "metadata": {
        "id": "viHQJLzYJTwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the alpha value of the model (intercept)\n",
        "print(\"Alpha/intercept (a)\")\n",
        "print(lin_model_fit.intercept_)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# print the beta values of the model (co-efficients)\n",
        "betas = lin_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients (b1 to b13)\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas[counter], 4)))\n",
        "    counter +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrdKjYmTJWpP",
        "outputId": "116ec207-8b4d-4ea6-9a06-2c4c3e8dda36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha/intercept (a)\n",
            "36.459488385090125\n",
            "\n",
            "\n",
            "Beta weights/co-efficients (b1 to b13)\n",
            "-----------------------------------------\n",
            "CRIM: -0.108\n",
            "ZN: 0.0464\n",
            "INDUS: 0.0206\n",
            "CHAS: 2.6867\n",
            "NOX: -17.7666\n",
            "RM: 3.8099\n",
            "AGE: 0.0007\n",
            "DIS: -1.4756\n",
            "RAD: 0.306\n",
            "TAX: -0.0123\n",
            "PTRATIO: -0.9527\n",
            "B: 0.0093\n",
            "LSTAT: -0.5248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also calculate the root mean squared error (RMSE) and the $R^2$ score:"
      ],
      "metadata": {
        "id": "mIIqlYGlJbUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict every Y value in the dataset\n",
        "boston_predict = lin_model_fit.predict(boston)\n",
        "\n",
        "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "rmse = (np.sqrt(mean_squared_error(boston_Y, boston_predict)))\n",
        "r2 = r2_score(boston_Y, boston_predict)\n",
        "\n",
        "# print the performance metrics\n",
        "print(\"Model performance\")\n",
        "print(\"--------------------------------------\")\n",
        "print(f'RMSE is {rmse}')\n",
        "print(f'R2 score is {r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNEanhiMJhgv",
        "outputId": "94f506f3-00df-4c82-c2ab-636e44874773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance\n",
            "--------------------------------------\n",
            "RMSE is 4.679191295697281\n",
            "R2 score is 0.7406426641094095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall this is a good model - \n",
        " scores above 0.7 are considered very much publishable/successful when building regression models! RMSE, the square root of the sum of each error term squared, is more domain specific (and often used for module comparison), but in this case means our forecasted property prices are incorrect by, on average, \\$4,679. Given that many properties sell for more than $50,000 this is not too bad.\n",
        "\n",
        "Keep scrolling, we'll next relook at this with a more \"machine learning\" mindset :)"
      ],
      "metadata": {
        "id": "9GpM1_PHJlp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning Approach\n",
        "We'll redo our linear regression and this time we'll make it more \"machine-learning-y\". Actually, arguably, all we need to do is via splitting our data into training and test:"
      ],
      "metadata": {
        "id": "00Vfvi8tJzqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training and test\n",
        "from sklearn.model_selection  import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(boston, boston_Y, test_size = 0.2)\n",
        "\n",
        "# print the shapes to check everything is OK\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLU_JYeFJ7bJ",
        "outputId": "5045ec44-1acd-4957-9259-81a0a0e6816a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13)\n",
            "(102, 13)\n",
            "(404,)\n",
            "(102,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"shapes\" shown here are number of rows by number of columns ... i.e. the shape of \"X_train\" is 404 rows by 13 columns. We'd expect the 13 columns (13 X's), and we can quickly do some math on the 506 row numbers ... \n",
        " ... to see that 404 is rougly 80 % of the data (our \"test_size\" was 20% so our training data should be 80%) and that this all looks correct. The Y \"shapes\" show only rows but this is because we have only 1x value (1x column).\n",
        "\n",
        "Using these subsets we can train the Linear Regression model on \"X_train\" and \"Y_train\", and then use this model to predict \"X_test\". If we can compare the predictions on \"X_test\" with the real values of \"Y_test\" this gives us a measure of how well our computer has learned to predict house prices:"
      ],
      "metadata": {
        "id": "NW_z8eufKDBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_model = LinearRegression()\n",
        "\n",
        "# fit the model to the training data\n",
        "lin_model_fit = lin_model.fit(X_train, Y_train)\n",
        "\n",
        "# predict the data\n",
        "boston_predict = lin_model_fit.predict(X_test)\n",
        "\n",
        "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "rmse = (np.sqrt(mean_squared_error(Y_test, boston_predict)))\n",
        "r2 = r2_score(Y_test, boston_predict)\n",
        "\n",
        "# print the performance metrics\n",
        "print(\"Model performance\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2soXyjhfKF1x",
        "outputId": "0fd38ecf-1065-498b-9642-05b8ba8cee25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance\n",
            "--------------------------------------\n",
            "RMSE is 4.313037645012264\n",
            "R2 score is 0.7566309647954168\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to the previous Notebook our RMSE is lower and \n",
        " higher (i.e. better in both cases)! This is a little suprising but probably just chance on how the data was split (random error means the metrics are superior). However, we can conclude that this hasn't made our model worse.\n",
        "\n",
        "What it has done, however, is meant we have a model that is tested against its ability to make predictions on unseen data ... i.e. that it has learned about the process of predicting house prices and can do this well."
      ],
      "metadata": {
        "id": "hxOOXeZfKJjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### L1 Regularisation (LASSO)\n",
        "$L1$ Regression is performed in much the same way as Linear Regression. We have one other value (a hyperparameter - more on these later) of $\\alpha$ which determines how much influence the \n",
        " penalty has on how the line is fit in the model. We will dodge the issue of what this should be for now and just randomly set it as 0.5"
      ],
      "metadata": {
        "id": "1eaFknfAKMCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "l1_model = Lasso(alpha=0.5)\n",
        "\n",
        "# fit the model to the training data\n",
        "l1_model_fit = l1_model.fit(X_train, Y_train)\n",
        "\n",
        "# predict the data\n",
        "boston_predict = l1_model_fit.predict(X_test)\n",
        "\n",
        "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "rmse = (np.sqrt(mean_squared_error(Y_test, boston_predict)))\n",
        "r2 = r2_score(Y_test, boston_predict)\n",
        "\n",
        "# print the performance metrics\n",
        "print(\"Model performance\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_7UJMGgKdUH",
        "outputId": "588fae85-25b4-4540-ae66-04679e023a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance\n",
            "--------------------------------------\n",
            "RMSE is 4.517395047468784\n",
            "R2 score is 0.7330223132991698\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance is slightly lower than for our normal linear model. However... this is somewhat to be expected and actually performance is only slightly worse. This may be a small price to pay for ensuring that our model is a little more robust to changing data.\n",
        "\n",
        "Where we should see a much more fundamental difference is in the beta-weights/coefficients of the two models (the $b$ values). Let's compare the two:"
      ],
      "metadata": {
        "id": "HA-xdePtKhGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the beta values of the model (co-efficients)\n",
        "betas = lin_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - UNREGULARISED\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas[counter], 4)))\n",
        "    counter +=1\n",
        "    \n",
        "print(\"\\n\")\n",
        "\n",
        "# print the beta values of the model (co-efficients)\n",
        "betas_l1 = l1_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - LASSO\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas_l1[counter], 4)))\n",
        "    counter +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAW2CdmaKlsL",
        "outputId": "60761fb1-c166-4112-878b-8de082354d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beta weights/co-efficients - UNREGULARISED\n",
            "-----------------------------------------\n",
            "CRIM: -0.1049\n",
            "ZN: 0.0571\n",
            "INDUS: 0.0576\n",
            "CHAS: 2.7284\n",
            "NOX: -20.8549\n",
            "RM: 3.3444\n",
            "AGE: 0.0104\n",
            "DIS: -1.5142\n",
            "RAD: 0.3726\n",
            "TAX: -0.0131\n",
            "PTRATIO: -1.0377\n",
            "B: 0.0093\n",
            "LSTAT: -0.5826\n",
            "\n",
            "\n",
            "Beta weights/co-efficients - LASSO\n",
            "-----------------------------------------\n",
            "CRIM: -0.0799\n",
            "ZN: 0.0627\n",
            "INDUS: -0.0\n",
            "CHAS: 0.0\n",
            "NOX: -0.0\n",
            "RM: 2.0658\n",
            "AGE: 0.0122\n",
            "DIS: -0.9604\n",
            "RAD: 0.3238\n",
            "TAX: -0.0157\n",
            "PTRATIO: -0.8232\n",
            "B: 0.0099\n",
            "LSTAT: -0.7091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see some of the beta weights are now 0 (\"INDUS\", \"CHAS\" and \"NOX\" - of which \"CHAS\" and \"NOX\" were fairly large weights in the original model). We have effectively removed these features (X's) from the model - they are now (for instance) $0 * INDUS$ which will obviously be zero and therefore won't change the calculation of $Y$. We have performed feature selection - removing X's from the model.\n",
        "\n",
        "We have also minimised the impact (weight) of all the X's. For instance, \"RM\" had a beta of 3.77 in the original model and only 0.74 in the LASSO model.\n",
        "\n",
        "Overall we have made the model less reliant on certain features and removed others entirely (reducing model complexity). Both these elements mean our model is more robust/protected against over-fitting ... and the cost is only 0.05 in terms of $R^2$ score."
      ],
      "metadata": {
        "id": "oDuK4R_jKsIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### L2 Regularisation (Ridge)\n",
        "$L2$ regularisation is performed in a very similar way in scikit-learn ... and again we will set the hyperparameter $\\alpha$ to the arbitrary value of 0.5:"
      ],
      "metadata": {
        "id": "7IrhbFSdLCXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "l2_model = Ridge(alpha=0.75)\n",
        "\n",
        "# fit the model to the training data\n",
        "l2_model_fit = l2_model.fit(X_train, Y_train)\n",
        "\n",
        "# predict the data\n",
        "boston_predict = l2_model_fit.predict(X_test)\n",
        "\n",
        "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "rmse = (np.sqrt(mean_squared_error(Y_test, boston_predict)))\n",
        "r2 = r2_score(Y_test, boston_predict)\n",
        "\n",
        "# print the performance metrics\n",
        "print(\"Model performance\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF7TGDvkLHuo",
        "outputId": "48bf3610-723f-4071-cd07-45efd53f38b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance\n",
            "--------------------------------------\n",
            "RMSE is 4.263553602085966\n",
            "R2 score is 0.7621833387754423\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have model performance that slightly improves upon the original model ... while also adding regularisation protection. Let's compare the three beta weights:"
      ],
      "metadata": {
        "id": "_y7nSsfLLLGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the beta values of the model (co-efficients)\n",
        "betas = lin_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - UNREGULARISED\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas[counter], 4)))\n",
        "    counter +=1\n",
        "    \n",
        "print(\"\\n\")\n",
        "\n",
        "# print the beta values of the model (co-efficients)\n",
        "betas_l1 = l1_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - LASSO\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas_l1[counter], 4)))\n",
        "    counter +=1\n",
        "    \n",
        "print(\"\\n\")\n",
        "\n",
        "# print the beta values of the model (co-efficients)\n",
        "betas_l2 = l2_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - RIDGE\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas_l2[counter], 4)))\n",
        "    counter +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FET5ez6oLPif",
        "outputId": "41276d8d-a585-4567-bda6-71503c11eb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beta weights/co-efficients - UNREGULARISED\n",
            "-----------------------------------------\n",
            "CRIM: -0.1049\n",
            "ZN: 0.0571\n",
            "INDUS: 0.0576\n",
            "CHAS: 2.7284\n",
            "NOX: -20.8549\n",
            "RM: 3.3444\n",
            "AGE: 0.0104\n",
            "DIS: -1.5142\n",
            "RAD: 0.3726\n",
            "TAX: -0.0131\n",
            "PTRATIO: -1.0377\n",
            "B: 0.0093\n",
            "LSTAT: -0.5826\n",
            "\n",
            "\n",
            "Beta weights/co-efficients - LASSO\n",
            "-----------------------------------------\n",
            "CRIM: -0.0799\n",
            "ZN: 0.0627\n",
            "INDUS: -0.0\n",
            "CHAS: 0.0\n",
            "NOX: -0.0\n",
            "RM: 2.0658\n",
            "AGE: 0.0122\n",
            "DIS: -0.9604\n",
            "RAD: 0.3238\n",
            "TAX: -0.0157\n",
            "PTRATIO: -0.8232\n",
            "B: 0.0099\n",
            "LSTAT: -0.7091\n",
            "\n",
            "\n",
            "Beta weights/co-efficients - RIDGE\n",
            "-----------------------------------------\n",
            "CRIM: -0.1028\n",
            "ZN: 0.0581\n",
            "INDUS: 0.0343\n",
            "CHAS: 2.6228\n",
            "NOX: -14.8011\n",
            "RM: 3.378\n",
            "AGE: 0.0051\n",
            "DIS: -1.4251\n",
            "RAD: 0.3587\n",
            "TAX: -0.0136\n",
            "PTRATIO: -0.978\n",
            "B: 0.0097\n",
            "LSTAT: -0.5878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with L1/LASSO regression all our beta values are lower than in the original model - and therefore each feature is less influential. However, none have been reduced to zero. L2 cannot perform feature selection (deleting X's) unlike with L1.\n",
        "\n",
        "So better performance and regularisation added - seems like a good deal right? However, before we call it a day we have one other regularisation method we may try."
      ],
      "metadata": {
        "id": "z4w6jxYQLU46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ElasticNet\n",
        "ElasticNet uses both $L1$ and $L2$ regularisation. We now have an additonal value to set (hyperparameter - still going to come back to these) the l1_ratio. As the name suggests the l1_ratio determines the proportion that is L1 versus L2 so \"1\" would be just L1 and \"0\" would just be L2. We'll arbitrarily go with 0.5 again."
      ],
      "metadata": {
        "id": "gtgXlvr4LafY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "enet_model = ElasticNet(alpha=0.5, l1_ratio=0.25)\n",
        "\n",
        "# fit the model to the training data\n",
        "enet_model_fit = enet_model.fit(X_train, Y_train)\n",
        "\n",
        "# predict the data\n",
        "boston_predict = enet_model_fit.predict(X_test)\n",
        "\n",
        "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "rmse = (np.sqrt(mean_squared_error(Y_test, boston_predict)))\n",
        "r2 = r2_score(Y_test, boston_predict)\n",
        "\n",
        "# print the performance metrics\n",
        "print(\"Model performance\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXbUNI6HLg74",
        "outputId": "230976c2-ccfb-4b79-cbca-bcdc723bd119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance\n",
            "--------------------------------------\n",
            "RMSE is 4.672882408344411\n",
            "R2 score is 0.7143274394848722\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsurprisingly the result is somewhere between the results of L1 and L2. Let's also check the weights:"
      ],
      "metadata": {
        "id": "4lhed1C_LpO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the beta values of the model (co-efficients)\n",
        "betas = lin_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - UNREGULARISED\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas[counter], 4)))\n",
        "    counter +=1\n",
        "    \n",
        "print(\"\\n\")\n",
        "\n",
        "# print the beta values of the model (co-efficients)\n",
        "betas_l1 = l1_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - LASSO\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas_l1[counter], 4)))\n",
        "    counter +=1\n",
        "    \n",
        "print(\"\\n\")\n",
        "\n",
        "# print the beta values of the model (co-efficients)\n",
        "betas_l2 = l2_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - RIDGE\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas_l2[counter], 4)))\n",
        "    counter +=1\n",
        "    \n",
        "print(\"\\n\")\n",
        "\n",
        "# print the beta values of the model (co-efficients)\n",
        "betas_enet = enet_model_fit.coef_\n",
        "counter = 0\n",
        "for col in boston.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - ELASTICNET\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(col + \": \" + str(round(betas_enet[counter], 4)))\n",
        "    counter +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYoweEwlLZJv",
        "outputId": "5e2d8d9c-50a7-4807-89b9-ac0778816957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beta weights/co-efficients - UNREGULARISED\n",
            "-----------------------------------------\n",
            "CRIM: -0.1049\n",
            "ZN: 0.0571\n",
            "INDUS: 0.0576\n",
            "CHAS: 2.7284\n",
            "NOX: -20.8549\n",
            "RM: 3.3444\n",
            "AGE: 0.0104\n",
            "DIS: -1.5142\n",
            "RAD: 0.3726\n",
            "TAX: -0.0131\n",
            "PTRATIO: -1.0377\n",
            "B: 0.0093\n",
            "LSTAT: -0.5826\n",
            "\n",
            "\n",
            "Beta weights/co-efficients - LASSO\n",
            "-----------------------------------------\n",
            "CRIM: -0.0799\n",
            "ZN: 0.0627\n",
            "INDUS: -0.0\n",
            "CHAS: 0.0\n",
            "NOX: -0.0\n",
            "RM: 2.0658\n",
            "AGE: 0.0122\n",
            "DIS: -0.9604\n",
            "RAD: 0.3238\n",
            "TAX: -0.0157\n",
            "PTRATIO: -0.8232\n",
            "B: 0.0099\n",
            "LSTAT: -0.7091\n",
            "\n",
            "\n",
            "Beta weights/co-efficients - RIDGE\n",
            "-----------------------------------------\n",
            "CRIM: -0.1028\n",
            "ZN: 0.0581\n",
            "INDUS: 0.0343\n",
            "CHAS: 2.6228\n",
            "NOX: -14.8011\n",
            "RM: 3.378\n",
            "AGE: 0.0051\n",
            "DIS: -1.4251\n",
            "RAD: 0.3587\n",
            "TAX: -0.0136\n",
            "PTRATIO: -0.978\n",
            "B: 0.0097\n",
            "LSTAT: -0.5878\n",
            "\n",
            "\n",
            "Beta weights/co-efficients - ELASTICNET\n",
            "-----------------------------------------\n",
            "CRIM: -0.0884\n",
            "ZN: 0.0681\n",
            "INDUS: -0.003\n",
            "CHAS: 0.1618\n",
            "NOX: -0.0\n",
            "RM: 1.4126\n",
            "AGE: 0.0179\n",
            "DIS: -0.9787\n",
            "RAD: 0.3658\n",
            "TAX: -0.0174\n",
            "PTRATIO: -0.8684\n",
            "B: 0.0095\n",
            "LSTAT: -0.7538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, as we may expect, the results are somewhere between the two. \"CHAS\" and \"NOX\" have betas of zero (have been removed) but not \"INDUS\". All other beta weights are reduced to somewhere between the L1 and L2 models.\n",
        "\n",
        "**BONUS! Which is best?**\n",
        "Hahahahaha ... you know what I'm going to say:\n",
        "\n",
        "## IT DEPENDS!\n",
        "If you are not worried about overfitting use the original model (but if you don't care about overfitting are you doing ML?). If you have a lot of features and want some removed you may use L1 or Elastic Net. If you care most about performance use the L2 model. In this case I'd probably go with L2 as there aren't a lot of features and the performance is best."
      ],
      "metadata": {
        "id": "Du6_D6N3LxYD"
      }
    }
  ]
}